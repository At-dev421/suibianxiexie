4.16 spider_one

---

[TOC]

---

# 1. 网站
&emsp;&emsp;选择的网站是[http://www.weather.com.cn/weather/101180101.shtml](https://cloud.tencent.com/developer/tools/blog-entry?target=http%3A%2F%2Fwww.weather.com.cn%2Fweather%2F101190401.shtml&source=article&objectId=2089228) 中国天气网中的郑州天气，准备抓取最近7天的天气以及最高/最低气温。

 ![image-20240416171547120](C:\Users\11247\AppData\Roaming\Typora\typora-user-images\image-20240416171547120.png)

## 1.1. 开发者模式

### 1.1.1header

header是request.get的一个参数，目的是模拟浏览器访问

header获取方法：打开chome，F12进入开发者工具--选择network(网络)--刷新网站，找到第一个网络请求，查看它的 Request Headers（请求标头），后续会用到其中的内容

![image-20240416204044612](C:\Users\11247\AppData\Roaming\Typora\typora-user-images\image-20240416204044612.png)

### 1.1.2获取html中我们需要的字段

还是开发者模式--element(元素)--找到class-7d的div的ul中--日期在每个li中h1 中，天气状况在每个li的第一个p标签内，最高温度和最低温度在每个li的span和i标签中（网页内容和源码对应显示，找网页中需要的内容即可）。注：**到了傍晚，当天气温会没有最高温度**

 *div是容器元素，定义文档的一个区域或部分，ul是列表元素，li标签是列表项目*

![image-20240416204458221](C:\Users\11247\AppData\Roaming\Typora\typora-user-images\image-20240416204458221.png)

# 2.代码

**编码声明**表示该程序采用utf-8编码，源程序中可以有中文而不会出现乱码

```python
# coding : UTF-8
```

*UTF-8是一种通用的Unicode字符编码方案，支持几乎所有的语言和符号*

## 2.1导入所需的库

```python
import requests
import csv
import random
import time
import socket
import http.client
from bs4 import BeautifulSoup
```

* `requests`：用来抓取网页的html源代码 
*  `csv`：将数据写入到csv文件中  
* `random`：取随机数  
* `time`：时间相关操作
* `socket`和`http.client` 在这里只用于异常处理  
* `BeautifulSoup`：用来代替正则式取源码中相应标签中的内容

## 2.2获取网页中的html代码和数据

### 2.2.1定义函数’get_content‘

它接受一个url和一个可选的数据参数

```python
def get_content(url , data = None):
```

#### 1.设置http请求标头信息

```python
header = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Encoding': 'gzip, deflate',
    'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
}

```

#### 2. 设置超时时间为一个随机数

timeout是设定的一个超时时间，取随机数是因为防止被网站认定为网络爬虫

```python
timeout = random.choice(range(80, 180))
```

#### 3.获取网页源代码，返回文本

在循环中，使用 `requests.get` 发送HTTP GET请求，传递URL和标头信息，并设置超时时间；

并将响应的编码设置为UTF-8，确保正确解析网页中的Unicode字符

如果成功获取响应，跳出循环

```python
while True:
    try:
		rep = requests.get(url, headers=header, timeout=timeout)
        rep.encoding = 'utf-8'
        break
```

> 异常处理，如 `socket.timeout` `socket.error`、`http.client.BadStatusLine` 和 `http.client.IncompleteRead`，并且在捕获到异常后，也是等待一段随机时间后继续尝试：

```python
	except socket.timeout as e:
    	print('3:', e)
    	time.sleep(random.choice(range(8, 15)))
	except socket.error as e:
    	print('4:', e)
    	time.sleep(random.choice(range(20, 60)))
	except http.client.BadStatusLine as e:
    	print('5:', e)
    	time.sleep(random.choice(range(30, 80)))
	except http.client.IncompleteRead as e:
    	print('6:', e)
    	time.sleep(random.choice(range(5, 15)))
```

最后返回响应内容的文本部分

```python
return rep.text
```

### 2.2.2定义函数’get_data‘,获取html中我们需要的字段（BeautifulSoup）

它接受一个 `HTML 文本`作为参数，并从中提取天气数据:

```python
def get_data(html_text):
```

使用`BeautifulSoup` 解析 HTML 文本:

```python
bs = BeautifulSoup(html_text, "html.parser")
```

#### 1.获取`<li>`元素

获取 `<body>` 部分：

```python
body = bs.body
```

找到 `id` 为 `'7d'` 的 `<div>` 元素：

```python
data = body.find('div',{'id':'7d'})
```

获取该 `<div>` 元素内的 `<ul>` 元素：

```python
ul = data.find('ul')
```

获取 `<ul>` 元素内所有的 `<li>` 元素：

```python
li = ul.find_all('li')
```

#### 2.遍历`<li>`元素

对于每个`<li>`元素，提取日期、天气、最高和最低气温，并添加到`final`列表中

```python
for day in li:
```

提取日期：

```python
data = day.find('h1').string
inf = day.find_all('p')
```

天气状况：

```python
weather_condition = inf[0].string
```

温度(当日傍晚没有最高温度)：

```python
temperature_highest = inf[1].find('span'.string).replace('℃'，'') if inf[1].find('span') else None
temperature_lowest = inf[1].find('i').string.replace('℃','')
```

添加到final表

```python
temp = [data,weather_condition,temperature_highest,temperature_lowest]
final.append(temp)
```

返回包含提取天气数据的 `final` 列表：

```python
return final
```

## 2.3写入`CSV`文件

定义函数 `write_data(data, name)`，它接受两个参数：要写入的数据 `data` 和文件名 `name`：

```python
def write_data(data, name):
```

将文件名存储到变量 `file_name` 中：

```python
	file_name = name
```

使用 `open` 函数打开文件，选项 `'a'` 表示以追加模式打开文件，`errors='ignore'` 表示在编码错误时忽略错误，`newline=''` 表示不插入额外的换行符：

```python
	with open(file_name, 'a', errors='ignore', newline='') as f:
```

使用 `csv.writer` 创建一个 CSV 写入对象 `f_csv`：

```python
		f_csv = csv.writer(f)
```

使用 `writerows` 将数据逐行写入文件：

```python
f_csv.writerows(data)
```

## 2.4主函数

```python
if __name__ == '__main__':
    url ='http://www.weather.com.cn/weather/101180101.shtml'
    html = get_content(url)
    result = get_data(html)
    write_data(result, 'weather.csv')
```

if ____name____ == '____main___'代码的作用是检查当前模块是否作为主程序直接执行。如果是，则执行下面缩进的代码块；如果不是，则代码块不会执行，这样可以防止代码在被导入时自动执行。

# 3.输出结果

![image-20240417101536837](C:\Users\11247\AppData\Roaming\Typora\typora-user-images\image-20240417101536837.png)
